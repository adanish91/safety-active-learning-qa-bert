{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca05cbf7-e330-4b2e-bf33-d2f0b744b69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SafetyBERT Self-Training QA System - Modified for Real Annotated Data\n",
    "====================================================================\n",
    "Uses real annotated data for initial training.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Core imports\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BertForQuestionAnswering, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.nn.functional import softmax\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: Load Real Annotated Data & Base Model Training\n",
    "# ============================================================================\n",
    "\n",
    "def load_annotated_data(file_path=\"annotated_data_merged_reviewed.csv\"):\n",
    "    \"\"\"Load real annotated data from CSV file\"\"\"\n",
    "    \n",
    "    print(f\"Loading annotated data from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded CSV with {len(df)} rows and columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        required_columns = ['narrative', 'body_part', 'work_activity', 'accident_cause']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"Missing required columns: {missing_columns}\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return []\n",
    "        \n",
    "        # Clean data - remove rows with missing narratives\n",
    "        df = df.dropna(subset=['narrative'])\n",
    "        df = df[df['narrative'].str.strip() != '']\n",
    "        \n",
    "        print(f\"After cleaning: {len(df)} rows with valid narratives\")\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        annotated_data = []\n",
    "        for idx, row in df.iterrows():\n",
    "            annotated_data.append({\n",
    "                'id': f\"annotated_{idx:04d}\",\n",
    "                'narrative': str(row['narrative']).strip(),\n",
    "                'body_part': str(row['body_part']).strip() if pd.notna(row['body_part']) else '',\n",
    "                'work_activity': str(row['work_activity']).strip() if pd.notna(row['work_activity']) else '',\n",
    "                'accident_cause': str(row['accident_cause']).strip() if pd.notna(row['accident_cause']) else ''\n",
    "            })\n",
    "        \n",
    "        print(f\"Converted {len(annotated_data)} annotated samples\")\n",
    "        \n",
    "        # Show statistics\n",
    "        body_part_count = sum(1 for item in annotated_data if item['body_part'] and item['body_part'] != 'nan')\n",
    "        work_activity_count = sum(1 for item in annotated_data if item['work_activity'] and item['work_activity'] != 'nan')\n",
    "        accident_cause_count = sum(1 for item in annotated_data if item['accident_cause'] and item['accident_cause'] != 'nan')\n",
    "        \n",
    "        print(f\"Answer statistics:\")\n",
    "        print(f\"Body part answers: {body_part_count}/{len(annotated_data)} ({body_part_count/len(annotated_data)*100:.1f}%)\")\n",
    "        print(f\"Work activity answers: {work_activity_count}/{len(annotated_data)} ({work_activity_count/len(annotated_data)*100:.1f}%)\")\n",
    "        print(f\"Accident cause answers: {accident_cause_count}/{len(annotated_data)} ({accident_cause_count/len(annotated_data)*100:.1f}%)\")\n",
    "        \n",
    "        return annotated_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading annotated data: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def convert_annotated_to_squad_format(annotated_data):\n",
    "    \"\"\"Convert annotated data to SQuAD v2.0 style QA format\"\"\"\n",
    "    \n",
    "    print(\"Converting annotated data to SQuAD v2.0 format...\")\n",
    "    \n",
    "    # Updated questions to match your requirements\n",
    "    questions = [\n",
    "        \"What body part was injured and what type of injury occurred?\",\n",
    "        \"What specific work activity was the employee performing when the accident occurred?\", \n",
    "        \"What was the cause of accident?\"\n",
    "    ]\n",
    "    \n",
    "    answer_keys = [\"body_part\", \"work_activity\", \"accident_cause\"]\n",
    "    \n",
    "    squad_data = []\n",
    "    stats = {\"with_answer\": 0, \"no_answer\": 0, \"multi_span\": 0}\n",
    "    \n",
    "    for item in annotated_data:\n",
    "        narrative = item[\"narrative\"]\n",
    "        \n",
    "        for question, answer_key in zip(questions, answer_keys):\n",
    "            answer_text = item[answer_key]\n",
    "            \n",
    "            # Debug print for first few examples\n",
    "            if len(squad_data) < 5:\n",
    "                print(f\"Debug - Question: {question}\")\n",
    "                print(f\"Debug - Answer text: '{answer_text}'\")\n",
    "                print(f\"Debug - In narrative: {answer_text in narrative if answer_text else False}\")\n",
    "            \n",
    "            if answer_text and answer_text.strip() != '' and answer_text.lower() != 'nan':\n",
    "                # Handle multiple answers (semicolon separated)\n",
    "                if ';' in answer_text:\n",
    "                    answer_texts = [ans.strip() for ans in answer_text.split(';') if ans.strip()]\n",
    "                    answer_starts = []\n",
    "                    valid_answers = []\n",
    "                    \n",
    "                    print(f\"Processing {len(answer_texts)} spans for {answer_key}\")\n",
    "                    \n",
    "                    for ans in answer_texts:\n",
    "                        # Try exact match first\n",
    "                        start_pos = narrative.find(ans)\n",
    "                        if start_pos == -1:\n",
    "                            # Try case-insensitive\n",
    "                            start_pos = narrative.lower().find(ans.lower())\n",
    "                            if start_pos != -1:\n",
    "                                # Use actual text from narrative to preserve case\n",
    "                                ans = narrative[start_pos:start_pos + len(ans)]\n",
    "                                print(f\"Case-corrected span: '{ans}'\")\n",
    "                        \n",
    "                        if start_pos != -1:\n",
    "                            # Verify the extraction\n",
    "                            extracted = narrative[start_pos:start_pos + len(ans)]\n",
    "                            if extracted != ans:\n",
    "                                print(f\"Extraction mismatch: expected '{ans}', got '{extracted}'\")\n",
    "                                ans = extracted  # Use what was actually extracted\n",
    "                            \n",
    "                            valid_answers.append(ans)\n",
    "                            answer_starts.append(start_pos)\n",
    "                            print(f\"Found span: '{ans}' at position {start_pos}\")\n",
    "                        else:\n",
    "                            print(f\"WARNING: Span '{ans}' not found in narrative\")\n",
    "                \n",
    "                    if valid_answers:\n",
    "                        squad_entry = {\n",
    "                            \"id\": f\"{item['id']}_{answer_key}\",\n",
    "                            \"context\": narrative,\n",
    "                            \"question\": question,\n",
    "                            \"question_type\": answer_key,\n",
    "                            \"answers\": {\n",
    "                                \"text\": valid_answers,\n",
    "                                \"answer_start\": answer_starts\n",
    "                            },\n",
    "                            \"is_impossible\": False\n",
    "                        }\n",
    "                        stats[\"with_answer\"] += 1\n",
    "                        stats[\"multi_span\"] += 1\n",
    "                        print(f\"Created multi-span entry with {len(valid_answers)} spans\")\n",
    "                    else:\n",
    "                        # No valid spans found, treat as no answer\n",
    "                        squad_entry = {\n",
    "                            \"id\": f\"{item['id']}_{answer_key}\",\n",
    "                            \"context\": narrative,\n",
    "                            \"question\": question,\n",
    "                            \"question_type\": answer_key,\n",
    "                            \"answers\": {\n",
    "                                \"text\": [],\n",
    "                                \"answer_start\": []\n",
    "                            },\n",
    "                            \"is_impossible\": True\n",
    "                        }\n",
    "                        stats[\"no_answer\"] += 1\n",
    "                        print(f\"No valid spans found, marked as impossible\")\n",
    "                \n",
    "                else:\n",
    "                    # Single answer (existing logic)\n",
    "                    if answer_text in narrative:\n",
    "                        answer_start = narrative.find(answer_text)\n",
    "                    else:\n",
    "                        # Try case-insensitive\n",
    "                        answer_start = narrative.lower().find(answer_text.lower())\n",
    "                        if answer_start != -1:\n",
    "                            # Use actual text from narrative to preserve case\n",
    "                            actual_answer = narrative[answer_start:answer_start + len(answer_text)]\n",
    "                            answer_text = actual_answer\n",
    "                    \n",
    "                    if answer_start != -1:\n",
    "                        # Verify the extraction\n",
    "                        extracted = narrative[answer_start:answer_start + len(answer_text)]\n",
    "                        if extracted != answer_text:\n",
    "                            print(f\"Extraction mismatch: expected '{answer_text}', got '{extracted}'\")\n",
    "                        \n",
    "                        squad_entry = {\n",
    "                            \"id\": f\"{item['id']}_{answer_key}\",\n",
    "                            \"context\": narrative,\n",
    "                            \"question\": question,\n",
    "                            \"question_type\": answer_key,\n",
    "                            \"answers\": {\n",
    "                                \"text\": [answer_text],\n",
    "                                \"answer_start\": [answer_start]\n",
    "                            },\n",
    "                            \"is_impossible\": False\n",
    "                        }\n",
    "                        stats[\"with_answer\"] += 1\n",
    "                    else:\n",
    "                        # No answer found\n",
    "                        squad_entry = {\n",
    "                            \"id\": f\"{item['id']}_{answer_key}\",\n",
    "                            \"context\": narrative,\n",
    "                            \"question\": question,\n",
    "                            \"question_type\": answer_key,\n",
    "                            \"answers\": {\n",
    "                                \"text\": [],\n",
    "                                \"answer_start\": []\n",
    "                            },\n",
    "                            \"is_impossible\": True\n",
    "                        }\n",
    "                        stats[\"no_answer\"] += 1\n",
    "            else:\n",
    "                # No answer case (SQuAD v2.0 format)\n",
    "                squad_entry = {\n",
    "                    \"id\": f\"{item['id']}_{answer_key}\",\n",
    "                    \"context\": narrative,\n",
    "                    \"question\": question,\n",
    "                    \"question_type\": answer_key,\n",
    "                    \"answers\": {\n",
    "                        \"text\": [],\n",
    "                        \"answer_start\": []\n",
    "                    },\n",
    "                    \"is_impossible\": True\n",
    "                }\n",
    "                stats[\"no_answer\"] += 1\n",
    "            \n",
    "            squad_data.append(squad_entry)\n",
    "    \n",
    "    print(f\"Created {len(squad_data)} QA pairs (SQuAD v2.0 format)\")\n",
    "    print(f\"With answers: {stats['with_answer']}\")\n",
    "    print(f\"Multi-span answers: {stats['multi_span']}\")\n",
    "    print(f\"No answers: {stats['no_answer']}\")\n",
    "    \n",
    "    return squad_data\n",
    "\n",
    "def create_train_val_test_split(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Split data into train/validation/test sets\"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating train/validation/test split...\")\n",
    "    \n",
    "    # Set seed for reproducible splits\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    n = len(data)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = train_end + int(n * val_ratio)\n",
    "    \n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "    \n",
    "    print(f\"Train: {len(train_data)} samples ({len(train_data)/n:.1%})\")\n",
    "    print(f\"Validation: {len(val_data)} samples ({len(val_data)/n:.1%})\")\n",
    "    print(f\"Test: {len(test_data)} samples ({len(test_data)/n:.1%})\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def setup_model_and_tokenizer(model_name=\"bert-base-uncased\"):\n",
    "    \"\"\"Setup BERT model and tokenizer for QA\"\"\"\n",
    "    \n",
    "    print(f\"\\nSetting up model and tokenizer: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        \n",
    "        print(f\"Model loaded on device: {device}\")\n",
    "        print(f\"Tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def preprocess_data(data, tokenizer, max_length=128):\n",
    "    \"\"\"Preprocess QA data for training (SQuAD v2.0 compatible)\"\"\"\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        contexts = examples[\"context\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized_examples = tokenizer(\n",
    "            questions,\n",
    "            contexts,\n",
    "            truncation=\"only_second\",\n",
    "            max_length=max_length,\n",
    "            stride=64,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        # Handle the offset mapping and sample mapping\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "        \n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "        \n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            \n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            \n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[\"answers\"][sample_index]\n",
    "            \n",
    "            # Check if this is an impossible question (SQuAD v2.0)\n",
    "            is_impossible = examples.get(\"is_impossible\", [False] * len(examples[\"question\"]))[sample_index]\n",
    "            \n",
    "            # For impossible questions or empty answers, point to CLS token\n",
    "            if is_impossible or len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "                \n",
    "                # Find the start and end of the context in the tokenized input\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != 1:\n",
    "                    token_start_index += 1\n",
    "                    \n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != 1:\n",
    "                    token_end_index -= 1\n",
    "                    \n",
    "                # Check if the answer is outside the context window\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    # Answer is outside context window, treat as impossible\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # Find the tokens corresponding to the answer\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    \n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "        \n",
    "        return tokenized_examples\n",
    "    \n",
    "    print(f\"\\nPreprocessing {len(data)} samples...\")\n",
    "    \n",
    "    dataset = Dataset.from_list(data)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Preprocessed {len(tokenized_dataset)} samples\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "def create_trainer(model, tokenizer, train_dataset, val_dataset):\n",
    "    \"\"\"Create trainer with early stopping\"\"\"\n",
    "    \n",
    "    print(\"\\nSetting up training configuration...\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_base_model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        # eval_steps=25,\n",
    "        save_strategy=\"epoch\", \n",
    "        # save_steps=25,\n",
    "        logging_steps=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=50,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=200,\n",
    "        learning_rate=3e-6,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None,\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: Unlabeled Data Preparation (Accident Narratives)\n",
    "# ============================================================================\n",
    "\n",
    "def load_accident_narratives(file_path=\"data-mill.csv\", narrative_column=\"narrative\"):\n",
    "    \"\"\"Load all accident narratives as a single prediction pool\"\"\"\n",
    "    \n",
    "    print(f\"\\nLoading accident narratives from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if narrative_column not in df.columns:\n",
    "            print(f\"Column '{narrative_column}' not found. Available columns: {list(df.columns)}\")\n",
    "            return []\n",
    "        \n",
    "        # Clean narratives\n",
    "        narratives = df[narrative_column].dropna().astype(str).tolist()\n",
    "        narratives = [n.strip() for n in narratives if len(n.strip()) > 10]  # Filter short narratives\n",
    "        \n",
    "        print(f\"Loaded {len(narratives)} narratives as prediction pool\")\n",
    "        print(f\"All narratives will be used for iterative self-training\")\n",
    "        \n",
    "        return narratives\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading narratives: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_questions_for_narratives(narratives):\n",
    "    \"\"\"Generate synthetic questions for accident narratives using updated question set\"\"\"\n",
    "    \n",
    "    print(f\"\\nGenerating questions for {len(narratives)} narratives...\")\n",
    "    \n",
    "    # Updated safety-specific question templates to match your requirements\n",
    "    question_templates = [\n",
    "        \"What body part was injured and what type of injury occurred?\",\n",
    "        \"What specific work activity was the employee performing when the accident occurred?\",\n",
    "        \"What was the cause of accident?\"\n",
    "    ]\n",
    "    \n",
    "    synthetic_qa_pairs = []\n",
    "    \n",
    "    for i, narrative in enumerate(narratives):\n",
    "        # Generate multiple questions per narrative\n",
    "        for template in question_templates:\n",
    "            synthetic_qa_pairs.append({\n",
    "                'id': f\"synthetic_{i}_{len(synthetic_qa_pairs)}\",\n",
    "                'question': template,\n",
    "                'context': narrative,\n",
    "                'narrative_id': i,\n",
    "                'is_synthetic': True,\n",
    "                'answers': {\n",
    "                    'text': [],\n",
    "                    'answer_start': []\n",
    "                },\n",
    "                'is_impossible': True \n",
    "            })\n",
    "    \n",
    "    print(f\"Generated {len(synthetic_qa_pairs)} synthetic QA pairs\")\n",
    "    print(f\"Average {len(synthetic_qa_pairs)/len(narratives):.1f} questions per narrative\")\n",
    "    \n",
    "    return synthetic_qa_pairs\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: Self-Training Implementation (UNCHANGED)\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_qa_confidence(start_logits, end_logits, predicted_answer, tokenizer):\n",
    "    \"\"\"Calculate confidence score for QA prediction\"\"\"\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    start_probs = F.softmax(start_logits, dim=-1)\n",
    "    end_probs = F.softmax(end_logits, dim=-1)\n",
    "    \n",
    "    # Get max probabilities\n",
    "    max_start_prob = torch.max(start_probs).item()\n",
    "    max_end_prob = torch.max(end_probs).item()\n",
    "    \n",
    "    # Combined probability confidence\n",
    "    prob_confidence = (max_start_prob * max_end_prob) ** 0.5\n",
    "    \n",
    "    # Answer quality heuristics\n",
    "    answer_tokens = tokenizer.tokenize(predicted_answer)\n",
    "    \n",
    "    # Length penalty (prefer 2-15 tokens)\n",
    "    length_penalty = 1.0\n",
    "    # if len(answer_tokens) < 2:\n",
    "    #     length_penalty = 0.3\n",
    "    # elif len(answer_tokens) > 15:\n",
    "    #     length_penalty = 0.7\n",
    "    \n",
    "    # Generic answer penalty\n",
    "    # generic_answers = [\"no\", \"yes\", \"unclear\", \"unknown\", \"none\", \"n/a\"]\n",
    "    # generic_penalty = 0.2 if predicted_answer.lower().strip() in generic_answers else 1.0\n",
    "\n",
    "    generic_penalty = 1\n",
    "    \n",
    "    # Empty answer penalty\n",
    "    # empty_penalty = 0.1 if len(predicted_answer.strip()) == 0 else 1.0\n",
    "\n",
    "    empty_penalty = 1\n",
    "    \n",
    "    \n",
    "    # Combined confidence score\n",
    "    final_confidence = prob_confidence * length_penalty * generic_penalty * empty_penalty\n",
    "    \n",
    "    return final_confidence\n",
    "\n",
    "def predict_with_confidence(model, tokenizer, qa_pairs, batch_size=16):\n",
    "    \"\"\"Generate predictions with confidence scores\"\"\"\n",
    "    \n",
    "    print(f\"\\nGenerating predictions for {len(qa_pairs)} QA pairs...\")\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(qa_pairs), batch_size):\n",
    "        batch = qa_pairs[i:i+batch_size]\n",
    "        \n",
    "        # Prepare batch inputs\n",
    "        questions = [item['question'] for item in batch]\n",
    "        contexts = [item['context'] for item in batch]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            contexts,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # EXTRACT offset_mapping BEFORE moving to device\n",
    "        offset_mappings = inputs.pop(\"offset_mapping\")\n",
    "        \n",
    "        # Now move the rest to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Process each item in batch\n",
    "        for j, item in enumerate(batch):\n",
    "            start_logits = outputs.start_logits[j]\n",
    "            end_logits = outputs.end_logits[j]\n",
    "            \n",
    "            # Get predicted answer indices\n",
    "            start_idx = torch.argmax(start_logits).item()\n",
    "            end_idx = torch.argmax(end_logits).item()\n",
    "            \n",
    "            # Extract using offset mapping (EXACT from original text)\n",
    "            if start_idx <= end_idx and start_idx < len(offset_mappings[j]) and end_idx < len(offset_mappings[j]):\n",
    "                # Get character positions in original text\n",
    "                start_char = offset_mappings[j][start_idx][0]\n",
    "                end_char = offset_mappings[j][end_idx][1]\n",
    "                \n",
    "                # Extract EXACT text from original context\n",
    "                predicted_answer = item['context'][start_char:end_char].strip()\n",
    "            else:\n",
    "                predicted_answer = \"\"\n",
    "            \n",
    "            # Calculate confidence\n",
    "            confidence = calculate_qa_confidence(start_logits, end_logits, predicted_answer, tokenizer)\n",
    "            \n",
    "            predictions.append({\n",
    "                'id': item['id'],\n",
    "                'question': item['question'],\n",
    "                'context': item['context'],\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'confidence': confidence,\n",
    "                'narrative_id': item.get('narrative_id', -1),\n",
    "                'start_idx': start_idx, \n",
    "                'end_idx': end_idx,      \n",
    "                'start_char': start_char if predicted_answer else -1,  # NEW: character positions\n",
    "                'end_char': end_char if predicted_answer else -1       # NEW: character positions\n",
    "            })\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + len(batch)}/{len(qa_pairs)} samples...\")\n",
    "    \n",
    "    print(f\"Generated {len(predictions)} predictions\")\n",
    "    return predictions\n",
    "\n",
    "def get_question_type(question):\n",
    "    \"\"\"Determine question type from question text - UPDATED for new questions\"\"\"\n",
    "    question_lower = question.lower().strip()\n",
    "    \n",
    "    if \"body part\" in question_lower and \"injury\" in question_lower:\n",
    "        return \"body_part\"\n",
    "    elif \"work activity\" in question_lower or \"performing\" in question_lower:\n",
    "        return \"work_activity\"\n",
    "    elif \"cause\" in question_lower and \"accident\" in question_lower:\n",
    "        return \"accident_cause\"\n",
    "    else:\n",
    "        # Fallback classification\n",
    "        return \"unknown\"\n",
    "\n",
    "def select_hybrid_samples(all_predictions, used_indices, max_samples=200):\n",
    "    \"\"\"\n",
    "    Hybrid selection: N/2 low confidence + N/2 high confidence samples\n",
    "    \"\"\"\n",
    "    print(f\"Selecting samples with hybrid strategy (low + high confidence)...\")\n",
    "    print(f\"Already used: {len(used_indices)} predictions\")\n",
    "    print(f\"Target samples: {max_samples} (split: {max_samples//2} low + {max_samples - max_samples//2} high)\")\n",
    "    \n",
    "    # Filter out used predictions and load previous reviews\n",
    "    previous_reviews = load_previous_reviews()\n",
    "    \n",
    "    available_predictions = []\n",
    "    for i, pred in enumerate(all_predictions):\n",
    "        original_idx = pred.get('original_index', i)\n",
    "        if original_idx not in used_indices:\n",
    "            if len(pred['predicted_answer'].strip()) > 0:  # Has answer\n",
    "                \n",
    "                # Check if this sample is already in training sets\n",
    "                sample_key = create_sample_key(pred['context'], pred['question'])\n",
    "                if sample_key in previous_reviews:\n",
    "                    review_data = previous_reviews[sample_key]\n",
    "                    if review_data.get('your_decision') == 'rejected':\n",
    "                        continue  # Skip rejected samples\n",
    "                    if review_data.get('reused', False) == True:\n",
    "                        continue  # Skip already used samples\n",
    "                \n",
    "                available_predictions.append((original_idx, pred))\n",
    "    \n",
    "    print(f\"Available predictions after filtering: {len(available_predictions)}\")\n",
    "    \n",
    "    if len(available_predictions) == 0:\n",
    "        print(\"No available predictions!\")\n",
    "        return []\n",
    "    \n",
    "    # Group by question type for balanced selection\n",
    "    by_question_type = defaultdict(list)\n",
    "    for idx, pred in available_predictions:\n",
    "        question_type = get_question_type(pred['question'])\n",
    "        if question_type != \"unknown\":\n",
    "            by_question_type[question_type].append((idx, pred))\n",
    "    \n",
    "    if len(by_question_type) == 0:\n",
    "        print(\"No valid question types found!\")\n",
    "        return []\n",
    "    \n",
    "    # Calculate samples per question type\n",
    "    available_types = list(by_question_type.keys())\n",
    "    samples_per_type = max_samples // len(available_types)\n",
    "    low_per_type = samples_per_type // 2\n",
    "    high_per_type = samples_per_type - low_per_type\n",
    "    \n",
    "    print(f\"\\nHybrid Strategy per question type:\")\n",
    "    print(f\"Total per type: {samples_per_type} ({low_per_type} low + {high_per_type} high confidence)\")\n",
    "    \n",
    "    selected_samples = []\n",
    "    \n",
    "    for question_type in sorted(available_types):\n",
    "        candidates = by_question_type[question_type]\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Sort by confidence for selection\n",
    "        candidates.sort(key=lambda x: x[1]['confidence'])\n",
    "        \n",
    "        # Apply diversity constraint\n",
    "        narrative_counts = defaultdict(int)\n",
    "        max_per_narrative = 3  # Reduced since we're splitting between low/high\n",
    "        \n",
    "        # Select LOW confidence samples (from start of sorted list)\n",
    "        low_conf_selected = []\n",
    "        for idx, pred in candidates:\n",
    "            if len(low_conf_selected) >= low_per_type:\n",
    "                break\n",
    "            narrative_id = pred['narrative_id']\n",
    "            if narrative_counts[narrative_id] >= max_per_narrative:\n",
    "                continue\n",
    "            low_conf_selected.append(pred)\n",
    "            narrative_counts[narrative_id] += 1\n",
    "        \n",
    "        # Reset narrative counts for high confidence selection\n",
    "        narrative_counts = defaultdict(int)\n",
    "        \n",
    "        # Select HIGH confidence samples (from end of sorted list)\n",
    "        high_conf_selected = []\n",
    "        for idx, pred in reversed(candidates):  # Start from highest confidence\n",
    "            if len(high_conf_selected) >= high_per_type:\n",
    "                break\n",
    "            narrative_id = pred['narrative_id']\n",
    "            if narrative_counts[narrative_id] >= max_per_narrative:\n",
    "                continue\n",
    "            # Only select high confidence samples above minimum threshold\n",
    "            if pred['confidence'] >= 0.6:  # Minimum threshold for \"high confidence\"\n",
    "                high_conf_selected.append(pred)\n",
    "                narrative_counts[narrative_id] += 1\n",
    "        \n",
    "        # Add selection info\n",
    "        if low_conf_selected or high_conf_selected:\n",
    "            all_low_confs = [s['confidence'] for s in low_conf_selected]\n",
    "            all_high_confs = [s['confidence'] for s in high_conf_selected]\n",
    "            \n",
    "            print(f\"{question_type}:\")\n",
    "            if low_conf_selected:\n",
    "                print(f\"  Low confidence: {len(low_conf_selected)} samples (range: {min(all_low_confs):.3f} - {max(all_low_confs):.3f})\")\n",
    "            if high_conf_selected:\n",
    "                print(f\"  High confidence: {len(high_conf_selected)} samples (range: {min(all_high_confs):.3f} - {max(all_high_confs):.3f})\")\n",
    "            \n",
    "            # Add confidence type flag for review process\n",
    "            for sample in low_conf_selected:\n",
    "                sample['selection_type'] = 'low_confidence'\n",
    "            for sample in high_conf_selected:\n",
    "                sample['selection_type'] = 'high_confidence'\n",
    "            \n",
    "            selected_samples.extend(low_conf_selected + high_conf_selected)\n",
    "        else:\n",
    "            print(f\"{question_type}: No samples selected\")\n",
    "    \n",
    "    print(f\"\\nTotal selected: {len(selected_samples)} samples\")\n",
    "    \n",
    "    # Show overall statistics\n",
    "    if selected_samples:\n",
    "        low_conf_total = sum(1 for s in selected_samples if s.get('selection_type') == 'low_confidence')\n",
    "        high_conf_total = sum(1 for s in selected_samples if s.get('selection_type') == 'high_confidence')\n",
    "        \n",
    "        print(f\"Final breakdown: {low_conf_total} low confidence + {high_conf_total} high confidence\")\n",
    "        \n",
    "        all_confidences = [s['confidence'] for s in selected_samples]\n",
    "        print(f\"Overall confidence range: {min(all_confidences):.3f} - {max(all_confidences):.3f}\")\n",
    "    \n",
    "    return selected_samples\n",
    "\n",
    "    \n",
    "def convert_to_training_format(selected_predictions):\n",
    "    \"\"\"Convert high-confidence predictions to training format\"\"\"\n",
    "    \n",
    "    training_samples = []\n",
    "    \n",
    "    for pred in selected_predictions:\n",
    "        training_samples.append({\n",
    "            'id': pred['id'],\n",
    "            'question': pred['question'],\n",
    "            'context': pred['context'],\n",
    "            'answers': {\n",
    "                'text': [pred['predicted_answer']],\n",
    "                'answer_start': [pred.get('start_idx', 0)]  # Approximate\n",
    "            },\n",
    "            'is_pseudo_labeled': True,\n",
    "            'confidence': pred['confidence']\n",
    "        })\n",
    "    \n",
    "    return training_samples\n",
    "\n",
    "def evaluate_model_qa(model, tokenizer, test_data, sample_name=\"Test Set\", print_wrong_predictions=False):\n",
    "    \"\"\"Complete evaluation with F1, Exact Match, and Fuzzy Match metrics\"\"\"\n",
    "    \n",
    "    import re\n",
    "    import string\n",
    "    from collections import Counter\n",
    "    from difflib import SequenceMatcher\n",
    "    \n",
    "    print(f\"\\nEvaluating {sample_name} with Complete Metrics...\")\n",
    "    \n",
    "    def normalize_answer(s, context=\"\"):\n",
    "        def remove_articles(text):\n",
    "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "        def fix_tokenization_spacing(text):\n",
    "            # Fix common tokenization issues\n",
    "            text = re.sub(r\"(\\w)\\s+\\'\\s*([a-z])\", r\"\\1'\\2\", text) \n",
    "            text = re.sub(r\"\\s+\\'\\s*s\\b\", \"'s\", text)              \n",
    "            return text\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "        \n",
    "        # Fix spacing FIRST, then normalize normally (keeping apostrophes!)\n",
    "        s = fix_tokenization_spacing(s)\n",
    "        return white_space_fix(remove_articles(lower(s)))\n",
    "    \n",
    "    def compute_f1(prediction, ground_truth, context=\"\"):\n",
    "        pred_tokens = normalize_answer(prediction, context).split()\n",
    "        truth_tokens = normalize_answer(ground_truth, context).split()\n",
    "        \n",
    "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "            return int(pred_tokens == truth_tokens)\n",
    "        \n",
    "        common_tokens = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "        num_common = sum(common_tokens.values())\n",
    "        \n",
    "        if num_common == 0:\n",
    "            return 0\n",
    "            \n",
    "        precision = num_common / len(pred_tokens)\n",
    "        recall = num_common / len(truth_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    def compute_exact_match(prediction, ground_truth, context=\"\"):\n",
    "        return normalize_answer(prediction, context) == normalize_answer(ground_truth, context)\n",
    "    \n",
    "    def compute_fuzzy_match(prediction, ground_truth, context=\"\", threshold=0.6):\n",
    "        \"\"\"Compute fuzzy string similarity using SequenceMatcher\"\"\"\n",
    "        pred_norm = normalize_answer(prediction, context)\n",
    "        truth_norm = normalize_answer(ground_truth, context)\n",
    "        \n",
    "        if not pred_norm and not truth_norm:\n",
    "            return 1.0\n",
    "        if not pred_norm or not truth_norm:\n",
    "            return 0.0\n",
    "            \n",
    "        similarity = SequenceMatcher(None, pred_norm, truth_norm).ratio()\n",
    "        return 1.0 if similarity >= threshold else 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    exact_matches = []\n",
    "    f1_scores = []\n",
    "    fuzzy_matches = []\n",
    "    wrong_predictions = []  # Store wrong predictions\n",
    "    \n",
    "    for i, example in enumerate(test_data):\n",
    "        inputs = tokenizer(\n",
    "            example[\"question\"],\n",
    "            example[\"context\"], \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        start_idx = torch.argmax(outputs.start_logits[0]).item()\n",
    "        end_idx = torch.argmax(outputs.end_logits[0]).item()\n",
    "        \n",
    "        if start_idx <= end_idx and start_idx < len(inputs[\"input_ids\"][0]) and end_idx < len(inputs[\"input_ids\"][0]):\n",
    "            predicted_answer = tokenizer.decode(\n",
    "                inputs[\"input_ids\"][0][start_idx:end_idx+1],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "        else:\n",
    "            predicted_answer = \"\"\n",
    "        \n",
    "        if example[\"answers\"][\"text\"]:\n",
    "            true_answers = example[\"answers\"][\"text\"] if isinstance(example[\"answers\"][\"text\"], list) else [example[\"answers\"][\"text\"]]\n",
    "        else:\n",
    "            true_answers = [\"\"]\n",
    "        \n",
    "        # Get context for evaluation\n",
    "        context = example.get('context', '')\n",
    "        \n",
    "        # Compute best scores across all possible answers\n",
    "        max_exact = 0\n",
    "        max_f1 = 0\n",
    "        max_fuzzy = 0\n",
    "        best_true_answer = true_answers[0]  # Keep track of best matching true answer\n",
    "        \n",
    "        for true_answer in true_answers:\n",
    "            exact = compute_exact_match(predicted_answer, true_answer, context)\n",
    "            f1 = compute_f1(predicted_answer, true_answer, context)\n",
    "            fuzzy = compute_fuzzy_match(predicted_answer, true_answer, context)\n",
    "            \n",
    "            # Update best scores and keep track of best true answer\n",
    "            if f1 > max_f1:\n",
    "                max_f1 = f1\n",
    "                best_true_answer = true_answer\n",
    "            if exact > max_exact:\n",
    "                max_exact = exact\n",
    "            if fuzzy > max_fuzzy:\n",
    "                max_fuzzy = fuzzy\n",
    "        \n",
    "        exact_matches.append(max_exact)\n",
    "        f1_scores.append(max_f1)\n",
    "        fuzzy_matches.append(max_fuzzy)\n",
    "        \n",
    "        # **COLLECT WRONG PREDICTIONS**\n",
    "        if max_exact == 0:  # If exact match is 0, it's wrong\n",
    "            wrong_predictions.append({\n",
    "                'question': example['question'],\n",
    "                'context': example['context'],\n",
    "                'true_answer': best_true_answer,\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'f1_score': max_f1,\n",
    "                'fuzzy_score': max_fuzzy,\n",
    "                'example_id': example.get('id', f'sample_{i}')\n",
    "            })\n",
    "        \n",
    "        # Show detailed examples for first few samples\n",
    "        if i < 5:\n",
    "            similarity_score = SequenceMatcher(None, normalize_answer(predicted_answer, context), normalize_answer(true_answers[0], context)).ratio()\n",
    "            status = \"CORRECT\" if max_exact == 1 else \"WRONG\"\n",
    "            print(f\"Sample {i+1} ({status}): Q: {example['question']}...\")\n",
    "            \n",
    "            context_display = example.get('context', 'No context available')\n",
    "            if len(context_display) > 200:\n",
    "                print(f\" Context: {context_display[:200]}...\")\n",
    "                print(f\"[...truncated from {len(context_display)} characters]\")\n",
    "            else:\n",
    "                print(f\"Context: {context_display}\")\n",
    "            \n",
    "            print(f\"Ground Truth: '{true_answers[0]}'\")\n",
    "            print(f\"Prediction:   '{predicted_answer}'\")\n",
    "            print(f\"EM: {max_exact} | F1: {max_f1:.3f} | Fuzzy: {max_fuzzy} | Sim: {similarity_score:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    # Calculate final scores\n",
    "    em_score = np.mean(exact_matches) * 100\n",
    "    f1_score = np.mean(f1_scores) * 100\n",
    "    fuzzy_score = np.mean(fuzzy_matches) * 100\n",
    "    \n",
    "    print(f\"{sample_name} Complete Results:\")\n",
    "    print(f\"F1 Score: {f1_score:.1f}%\")\n",
    "    print(f\"Exact Match: {em_score:.1f}%\") \n",
    "    print(f\"Fuzzy Match: {fuzzy_score:.1f}%\")\n",
    "    print(f\"Total Questions: {len(test_data)}\")\n",
    "    print(f\"Wrong Predictions: {len(wrong_predictions)}\")\n",
    "    \n",
    "    # **PRINT WRONG PREDICTIONS IF REQUESTED**\n",
    "    if print_wrong_predictions and len(wrong_predictions) > 0:\n",
    "        print(f\"\\nWRONG PREDICTIONS FOR {sample_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, wrong in enumerate(wrong_predictions):\n",
    "            print(f\"\\nWrong Prediction #{i+1} (ID: {wrong['example_id']})\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # Print context (truncate if too long)\n",
    "            context = wrong['context']\n",
    "            if len(context) > 300:\n",
    "                print(f\"CONTEXT: {context[:300]}...\")\n",
    "                print(f\"[...truncated from {len(context)} characters]\")\n",
    "            else:\n",
    "                print(f\"CONTEXT: {context}\")\n",
    "            \n",
    "            print(f\"QUESTION: {wrong['question']}\")\n",
    "            print(f\"TRUE ANSWER: '{wrong['true_answer']}'\")\n",
    "            print(f\"PREDICTED: '{wrong['predicted_answer']}'\")\n",
    "            print(f\"F1: {wrong['f1_score']:.3f} | Fuzzy: {wrong['fuzzy_score']:.3f}\")\n",
    "            \n",
    "            # Add separator between examples\n",
    "            if i < len(wrong_predictions) - 1:\n",
    "                print()\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Showed {len(wrong_predictions)} wrong predictions out of {len(test_data)} total\")\n",
    "    \n",
    "    # Return F1 as primary metric\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def create_sample_key(narrative, question):\n",
    "    \"\"\"Create a unique key for narrative + question combination\"\"\"\n",
    "    narrative_snippet = narrative.strip()\n",
    "    return f\"{narrative_snippet}|||{question.strip()}\"\n",
    "\n",
    "def load_previous_reviews(review_file=\"reviewed_samples.json\"):\n",
    "    \"\"\"Load previous reviews and create lookup dictionary\"\"\"\n",
    "    if not os.path.exists(review_file):\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        with open(review_file, 'r') as f:\n",
    "            reviews = json.load(f)\n",
    "        \n",
    "        # Create lookup dictionary: sample_key -> review_data\n",
    "        lookup = {}\n",
    "        for review_key, review_data in reviews.items():\n",
    "            sample_key = create_sample_key(review_data['narrative'], review_data['question'])\n",
    "            lookup[sample_key] = review_data\n",
    "        \n",
    "        return lookup\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading previous reviews: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_single_review(review_entry, iteration, sample_index, review_file=\"reviewed_samples.json\"):\n",
    "    \"\"\"Save a single review immediately to prevent data loss\"\"\"\n",
    "    try:\n",
    "        # Load existing reviews\n",
    "        if os.path.exists(review_file):\n",
    "            with open(review_file, 'r') as f:\n",
    "                existing_reviews = json.load(f)\n",
    "        else:\n",
    "            existing_reviews = {}\n",
    "        \n",
    "        # Create unique key for this review\n",
    "        review_key = f\"iter_{iteration}_sample_{sample_index}_{datetime.now().strftime('%H%M%S')}\"\n",
    "        existing_reviews[review_key] = review_entry\n",
    "        \n",
    "        # Save immediately\n",
    "        with open(review_file, 'w') as f:\n",
    "            json.dump(existing_reviews, f, indent=2)\n",
    "        \n",
    "        print(f\"Review saved: {review_key}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving review: {e}\")\n",
    "        return False\n",
    "\n",
    "def update_existing_review_reused_status(sample_key, iteration, review_file=\"reviewed_samples.json\"):\n",
    "    \"\"\"Update existing review entry's reused status without creating duplicates\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(review_file):\n",
    "            with open(review_file, 'r') as f:\n",
    "                all_reviews = json.load(f)\n",
    "            \n",
    "            # Find and update the existing entry\n",
    "            for key, review in all_reviews.items():\n",
    "                existing_key = create_sample_key(review['narrative'], review['question'])\n",
    "                if existing_key == sample_key:\n",
    "                    all_reviews[key]['reused'] = True\n",
    "                    all_reviews[key]['last_used_iteration'] = iteration\n",
    "                    # print(f\"Updated existing review: {key}\")\n",
    "                    break\n",
    "            \n",
    "            # Save back\n",
    "            with open(review_file, 'w') as f:\n",
    "                json.dump(all_reviews, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating review: {e}\")\n",
    "\n",
    "def interactive_review(hybrid_samples, iteration, all_synthetic_qa_pairs):\n",
    "    \"\"\"\n",
    "    Interactive review with reuse analysis per question type\n",
    "    Returns: (approved_samples, all_processed_indices)\n",
    "    \"\"\"\n",
    "    print(f\"\\nINTERACTIVE REVIEW - Iteration {iteration}\")\n",
    "    print(f\"Processing {len(hybrid_samples)} pre-selected samples\")\n",
    "    \n",
    "    # Load previous reviews\n",
    "    previous_reviews = load_previous_reviews()\n",
    "    print(f\"Loaded {len(previous_reviews)} previous reviews for reuse\")\n",
    "    \n",
    "    # Group hybrid_samples by question type\n",
    "    samples_by_type = defaultdict(list)\n",
    "    for sample in hybrid_samples:\n",
    "        question_type = get_question_type(sample['question'])\n",
    "        samples_by_type[question_type].append(sample)\n",
    "    \n",
    "    print(f\"\\nPre-selected samples by question type:\")\n",
    "    for question_type, samples in samples_by_type.items():\n",
    "        if len(samples) > 0:\n",
    "            confidences = [s['confidence'] for s in samples]\n",
    "            print(f\"{question_type}: {len(samples)} samples \"\n",
    "                  f\"(confidence: {max(confidences):.2f} - {min(confidences):.2f})\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STEP 2: ANALYZE REUSED VS NEW FOR EACH QUESTION TYPE\n",
    "    # ============================================================================\n",
    "    print(f\"\\nREUSE ANALYSIS:\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    approved_samples = []\n",
    "    review_plan = {}\n",
    "    total_reused = 0\n",
    "    total_to_review = 0\n",
    "    total_reviewed = 0\n",
    "    reused_original_indices = []  # Track original indices for reused samples\n",
    "    new_reviewed_indices = []     # Track original indices for newly reviewed samples\n",
    "    \n",
    "    for question_type, samples in samples_by_type.items():\n",
    "        if len(samples) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check which samples are already reviewed\n",
    "        reused_samples = []\n",
    "        new_samples = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            sample_key = create_sample_key(sample['context'], sample['question'])\n",
    "            \n",
    "            if sample_key in previous_reviews:\n",
    "                previous_review = previous_reviews[sample_key]\n",
    "                # Only reuse if previously accepted/corrected\n",
    "                if previous_review['your_decision'] in ['accepted', 'corrected']:\n",
    "                    reused_samples.append((sample, previous_review))\n",
    "                elif previous_review['your_decision'] == 'rejected':\n",
    "                    continue  # Skip rejected samples\n",
    "                else:\n",
    "                    new_samples.append(sample)  # Unknown decision, treat as new\n",
    "            else:\n",
    "                new_samples.append(sample)\n",
    "        \n",
    "        # Store plan for this question type\n",
    "        review_plan[question_type] = {\n",
    "            'target': len(samples),\n",
    "            'reused': reused_samples,\n",
    "            'new': new_samples,\n",
    "            'reused_count': len(reused_samples),\n",
    "            'new_count': len(new_samples)\n",
    "        }\n",
    "        \n",
    "        total_reused += len(reused_samples)\n",
    "        total_to_review += len(new_samples)\n",
    "    \n",
    "    print(f\"\\nTOTAL: {total_reused} reused + {total_to_review} new = {total_reused + total_to_review} samples\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PROCESS REUSED SAMPLES (AUTOMATIC) + TRACK ORIGINAL INDICES\n",
    "    # ============================================================================\n",
    "    if total_reused > 0:\n",
    "        print(f\"\\nProcessing {total_reused} reused samples...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        reused_count = 0\n",
    "        for question_type, plan in review_plan.items():\n",
    "            for sample, previous_review in plan['reused']:\n",
    "                reused_count += 1\n",
    "                sample_key = create_sample_key(sample['context'], sample['question'])\n",
    "                \n",
    "                decision = previous_review['your_decision']\n",
    "                \n",
    "                if decision == 'accepted':\n",
    "                    approved_samples.append(sample)\n",
    "                    \n",
    "                elif decision == 'corrected':\n",
    "                    corrected_sample = sample.copy()\n",
    "                    corrected_sample['predicted_answer'] = previous_review['final_answer']\n",
    "                    corrected_sample['original_answer'] = sample['predicted_answer']\n",
    "                    corrected_sample['human_corrected'] = True\n",
    "                    corrected_sample['reused_correction'] = True\n",
    "                    \n",
    "                    if previous_review['final_answer'] == \"\":\n",
    "                        corrected_sample['is_no_answer'] = True\n",
    "                    \n",
    "                    approved_samples.append(corrected_sample)\n",
    "                \n",
    "                # Find and track original index for this reused sample\n",
    "                for i, original_qa in enumerate(all_synthetic_qa_pairs):\n",
    "                    original_key = create_sample_key(original_qa['context'], original_qa['question'])\n",
    "                    if original_key == sample_key:\n",
    "                        reused_original_indices.append(i)\n",
    "                        break\n",
    "                \n",
    "                # Update reused status (only for actually reused samples)\n",
    "                update_existing_review_reused_status(sample_key, iteration)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # REVIEW NEW SAMPLES BY QUESTION TYPE\n",
    "    # ============================================================================\n",
    "    if total_to_review > 0:\n",
    "        print(f\"\\nReviewing {total_to_review} new samples by question type...\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Options:\")\n",
    "        print(\"(a)ccept - Use model's answer as-is\")\n",
    "        print(\"(c)orrect - Provide correct answer (or press Enter for 'no answer')\")\n",
    "        print(\"(r)eject - Don't use this sample for training\")\n",
    "        print(\"(s)kip - Skip remaining samples and continue\")\n",
    "        print(\"(q)uit - Quit and save progress\")\n",
    "        print(\"=\" * 60)        \n",
    "        \n",
    "        for question_type in sorted(review_plan.keys()):\n",
    "            plan = review_plan[question_type]\n",
    "            new_samples = plan['new']\n",
    "            \n",
    "            if len(new_samples) == 0:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n=== REVIEWING {question_type.upper()} ({len(new_samples)} samples) ===\")\n",
    "            \n",
    "            # Sort new samples by confidence (lowest first for review)\n",
    "            new_samples.sort(key=lambda x: x['confidence'], reverse=False)\n",
    "            \n",
    "            type_reviewed = 0\n",
    "            for sample in new_samples:\n",
    "                total_reviewed += 1\n",
    "                type_reviewed += 1\n",
    "                \n",
    "                print(f\"\\nSample {type_reviewed}/{len(new_samples)} for {question_type}\")\n",
    "                print(f\"Global progress: {total_reviewed}/{total_to_review} | Confidence: {sample['confidence']:.3f}\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "                # Display context\n",
    "                narrative = sample['context']\n",
    "                if len(narrative) > 500:\n",
    "                    print(f\"CONTEXT: {narrative[:500]}...\")\n",
    "                    print(f\"[...truncated from {len(narrative)} characters]\")\n",
    "                else:\n",
    "                    print(f\"CONTEXT: {narrative}\")\n",
    "                \n",
    "                print(f\"\\nQUESTION: {sample['question']}\")\n",
    "                print(f\"MODEL ANSWER: \\\"{sample['predicted_answer']}\\\"\")\n",
    "                print(f\"CONFIDENCE: {sample['confidence']:.3f}\")\n",
    "                \n",
    "                # Get user input\n",
    "                while True:\n",
    "                    choice = input(f\"\\nChoice (a/c/r/s/q): \").lower().strip()\n",
    "                    \n",
    "                    if choice == 'a':  # Accept\n",
    "                        approved_samples.append(sample)\n",
    "                        print(\"? Accepted\")\n",
    "                        decision = \"accepted\"\n",
    "                        final_answer = sample['predicted_answer']\n",
    "                        \n",
    "                        # NEW: Track original index for newly reviewed sample\n",
    "                        sample_key = create_sample_key(sample['context'], sample['question'])\n",
    "                        for i, original_qa in enumerate(all_synthetic_qa_pairs):\n",
    "                            original_key = create_sample_key(original_qa['context'], original_qa['question'])\n",
    "                            if original_key == sample_key:\n",
    "                                new_reviewed_indices.append(i)\n",
    "                                break\n",
    "                        break\n",
    "                        \n",
    "                    elif choice == 'c':  # Correct\n",
    "                        print(\"\\nEnter correct answer (or press Enter for 'no answer'):\")\n",
    "                        corrected_answer = input(\"Correct answer: \").strip()\n",
    "                        \n",
    "                        corrected_sample = sample.copy()\n",
    "                        corrected_sample['predicted_answer'] = corrected_answer\n",
    "                        corrected_sample['original_answer'] = sample['predicted_answer']\n",
    "                        corrected_sample['human_corrected'] = True\n",
    "                        \n",
    "                        if corrected_answer == \"\":\n",
    "                            corrected_sample['is_no_answer'] = True\n",
    "                            print(\"? Corrected to: NO ANSWER\")\n",
    "                        else:\n",
    "                            corrected_sample['is_no_answer'] = False\n",
    "                            print(f\"? Corrected to: \\\"{corrected_answer}\\\"\")\n",
    "                        \n",
    "                        approved_samples.append(corrected_sample)\n",
    "                        decision = \"corrected\"\n",
    "                        final_answer = corrected_answer\n",
    "                        \n",
    "                        # NEW: Track original index for newly reviewed sample\n",
    "                        sample_key = create_sample_key(sample['context'], sample['question'])\n",
    "                        for i, original_qa in enumerate(all_synthetic_qa_pairs):\n",
    "                            original_key = create_sample_key(original_qa['context'], original_qa['question'])\n",
    "                            if original_key == sample_key:\n",
    "                                new_reviewed_indices.append(i)\n",
    "                                break\n",
    "                        break\n",
    "                        \n",
    "                    elif choice == 'r':  # Reject\n",
    "                        print(\"? Rejected\")\n",
    "                        decision = \"rejected\"\n",
    "                        final_answer = \"\"\n",
    "                        \n",
    "                        # NEW: Still track rejected samples as \"used\" so they won't be selected again\n",
    "                        sample_key = create_sample_key(sample['context'], sample['question'])\n",
    "                        for i, original_qa in enumerate(all_synthetic_qa_pairs):\n",
    "                            original_key = create_sample_key(original_qa['context'], original_qa['question'])\n",
    "                            if original_key == sample_key:\n",
    "                                new_reviewed_indices.append(i)\n",
    "                                break\n",
    "                        break\n",
    "                        \n",
    "                    elif choice == 's':  # Skip remaining\n",
    "                        print(f\"Skipping remaining samples.\")\n",
    "                        print_current_summary(approved_samples, total_reused, total_reviewed, total_to_review)\n",
    "                        # Return ALL processed indices (reused + newly reviewed so far)\n",
    "                        return approved_samples, reused_original_indices + new_reviewed_indices\n",
    "                        \n",
    "                    elif choice == 'q':  # Quit\n",
    "                        print(\"Quitting review process\")\n",
    "                        continue_choice = input(\"Continue training with current samples (y/n): \").lower().strip()\n",
    "                        if continue_choice == 'y':\n",
    "                            # Return ALL processed indices (reused + newly reviewed so far)\n",
    "                            return approved_samples, reused_original_indices + new_reviewed_indices\n",
    "                        else:\n",
    "                            return [], []\n",
    "                            \n",
    "                    else:\n",
    "                        print(\"Invalid choice. Please enter 'a', 'c', 'r', 's', or 'q'\")\n",
    "                \n",
    "                # Save review immediately for new samples\n",
    "                sample_key = create_sample_key(sample['context'], sample['question'])\n",
    "                reused_value = decision in ['accepted', 'corrected']  # True if approved, False if rejected\n",
    "                \n",
    "                review_entry = {\n",
    "                    \"narrative\": narrative,\n",
    "                    \"question\": sample['question'],\n",
    "                    \"model_answer\": sample['predicted_answer'],\n",
    "                    \"your_decision\": decision,\n",
    "                    \"final_answer\": final_answer,\n",
    "                    \"confidence\": sample['confidence'],\n",
    "                    \"iteration\": iteration,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"reused\": reused_value,  # Set to True for approved samples, False for rejected\n",
    "                    \"question_type\": question_type\n",
    "                }\n",
    "                save_single_review(review_entry, iteration, total_reviewed)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # FINAL SUMMARY\n",
    "    # ============================================================================\n",
    "    print(f\"\\nFINAL PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total reused samples processed: {total_reused}\")\n",
    "    print(f\"Total new samples reviewed: {total_reviewed}\")\n",
    "    print(f\"Total approved samples for training: {len(approved_samples)}\")\n",
    "    print(f\"Original indices for reused samples: {len(reused_original_indices)}\")\n",
    "    print(f\"Original indices for new reviewed samples: {len(new_reviewed_indices)}\")\n",
    "    \n",
    "    # Show breakdown of approved samples\n",
    "    reused_approved = sum(1 for s in approved_samples if s.get('reused_correction', False) or 'human_corrected' not in s)\n",
    "    new_approved = len(approved_samples) - reused_approved\n",
    "    \n",
    "    print(f\"Approved samples breakdown:\")\n",
    "    print(f\"From reused: {reused_approved}\")\n",
    "    print(f\"From new: {new_approved}\")\n",
    "    print(f\"Total: {len(approved_samples)}\")\n",
    "    \n",
    "    all_processed_indices = reused_original_indices + new_reviewed_indices\n",
    "    if all_processed_indices:\n",
    "        print(f\"All processed indices (reused + new): {len(all_processed_indices)}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print_current_summary(approved_samples, total_reused, total_reviewed, total_to_review)\n",
    "    \n",
    "    # Return approved samples AND ALL processed indices (both reused and newly reviewed)\n",
    "    return approved_samples, all_processed_indices\n",
    "\n",
    "def convert_reviewed_samples_to_training_format(reviewed_samples):\n",
    "    \"\"\"\n",
    "    Convert reviewed samples to proper training format with answer_text and answer_start\n",
    "    \"\"\"\n",
    "    converted_samples = []\n",
    "    \n",
    "    for sample in reviewed_samples:\n",
    "        # Get the final answer (could be corrected or original)\n",
    "        if sample.get('human_corrected', False):\n",
    "            final_answer = sample['predicted_answer']  # This is the corrected answer\n",
    "        else:\n",
    "            final_answer = sample['predicted_answer']  # This is the accepted answer\n",
    "        \n",
    "        # Create training format sample\n",
    "        training_sample = {\n",
    "            'id': sample.get('id', 'unknown'),\n",
    "            'context': sample['context'],\n",
    "            'question': sample['question'],\n",
    "            'answer_text': '',\n",
    "            'answer_start': -1,\n",
    "            'confidence': sample.get('confidence', 1.0), \n",
    "            'human_corrected': sample.get('human_corrected', False),  \n",
    "            'original_answer': sample.get('original_answer', ''),  \n",
    "        }\n",
    "        \n",
    "        # Calculate answer_start index\n",
    "        if final_answer and final_answer.strip() != '':\n",
    "            context = sample['context']\n",
    "            \n",
    "            # Try exact match first (case-sensitive)\n",
    "            answer_start = context.find(final_answer)\n",
    "            \n",
    "            if answer_start != -1:\n",
    "                # Perfect match - use as-is\n",
    "                training_sample['answer_start'] = answer_start\n",
    "                training_sample['answer_text'] = final_answer\n",
    "            else:\n",
    "                # Try case-insensitive search\n",
    "                answer_start = context.lower().find(final_answer.lower())\n",
    "                if answer_start != -1:\n",
    "                    # Extract exact text from context to preserve original case\n",
    "                    actual_text_from_context = context[answer_start:answer_start + len(final_answer)]\n",
    "                    training_sample['answer_start'] = answer_start\n",
    "                    training_sample['answer_text'] = actual_text_from_context  # Use context's exact case!\n",
    "                    print(f\"Case-corrected: '{final_answer}' '{actual_text_from_context}'\")\n",
    "                else:\n",
    "                    # Try fuzzy matching for minor differences\n",
    "                    import re\n",
    "                    # Remove extra spaces and punctuation for matching\n",
    "                    clean_answer = re.sub(r'[^\\w\\s]', '', final_answer.strip())\n",
    "                    \n",
    "                    # Search for similar text (allowing for minor variations)\n",
    "                    pattern = re.escape(clean_answer).replace(r'\\ ', r'\\s+')\n",
    "                    match = re.search(pattern, context, re.IGNORECASE)\n",
    "                    \n",
    "                    if match:\n",
    "                        training_sample['answer_start'] = match.start()\n",
    "                        training_sample['answer_text'] = context[match.start():match.end()]\n",
    "                        print(f\"Fuzzy-matched: '{final_answer}' '{training_sample['answer_text']}'\")\n",
    "                    else:\n",
    "                        print(f\"WARNING: Answer '{final_answer}' not found in context for {sample.get('id', 'unknown')}\")\n",
    "                        training_sample['answer_start'] = -1\n",
    "                        training_sample['answer_text'] = ''  # Mark as no answer\n",
    "        else:\n",
    "            # No answer case\n",
    "            training_sample['answer_text'] = ''\n",
    "            training_sample['answer_start'] = -1\n",
    "        \n",
    "        converted_samples.append(training_sample)\n",
    "    \n",
    "    return converted_samples\n",
    "\n",
    "def print_current_summary(approved_samples, reused_count, reviewed_count, total_new):\n",
    "    \"\"\"Print current review summary\"\"\"\n",
    "    print(f\"\\nCURRENT REVIEW SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Reused previous decisions: {reused_count}\")\n",
    "    print(f\"Manually reviewed: {reviewed_count}/{total_new}\")\n",
    "    \n",
    "    if approved_samples:\n",
    "        no_answer_approved = sum(1 for sample in approved_samples if sample.get('is_no_answer', False))\n",
    "        with_answer_approved = len(approved_samples) - no_answer_approved\n",
    "        print(f\"Total approved: {len(approved_samples)}\")\n",
    "        print(f\"- With answers: {with_answer_approved}\")\n",
    "        print(f\"- No answer (empty): {no_answer_approved}\")\n",
    "    else:\n",
    "        print(\"Total approved: 0\")   \n",
    "\n",
    "\n",
    "def convert_to_training_format(selected_predictions):\n",
    "    \"\"\"Convert high-confidence predictions to training format with question type tracking\"\"\"\n",
    "    \n",
    "    training_samples = []\n",
    "    \n",
    "    for pred in selected_predictions:\n",
    "        # Check if this is a no-answer case\n",
    "        is_no_answer = pred.get('is_no_answer', False)\n",
    "        predicted_answer = pred.get('answer_text', '')\n",
    "        context = pred['context']\n",
    "        question_type = get_question_type(pred['question'])  # Add question type tracking\n",
    "        \n",
    "        if is_no_answer or predicted_answer == \"\":\n",
    "            # No answer case - SQuAD v2.0 format\n",
    "            training_sample = {\n",
    "                'id': pred['id'],\n",
    "                'question': pred['question'],\n",
    "                'context': context,\n",
    "                'answers': {\n",
    "                    'text': [],           # Empty list for no answer\n",
    "                    'answer_start': []    # Empty list for no answer\n",
    "                },\n",
    "                'is_impossible': True,    # SQuAD v2.0 flag for no answer\n",
    "                'is_pseudo_labeled': True,\n",
    "                'confidence': pred['confidence'],\n",
    "                'human_reviewed': pred.get('human_corrected', False),\n",
    "                'question_type': question_type  # Add question type\n",
    "            }\n",
    "        else:\n",
    "            # Has answer case - need to find correct position in context\n",
    "            answer_start = pred.get('answer_start', -1)\n",
    "            \n",
    "            # If answer_start is missing or invalid, find it properly\n",
    "            if answer_start == -1:\n",
    "                # Try exact match first (case-sensitive)\n",
    "                answer_start = context.find(predicted_answer)\n",
    "                \n",
    "                if answer_start == -1:\n",
    "                    # Try case-insensitive search\n",
    "                    answer_start = context.lower().find(predicted_answer.lower())\n",
    "                    if answer_start != -1:\n",
    "                        # Extract actual text from context to preserve case\n",
    "                        actual_answer = context[answer_start:answer_start + len(predicted_answer)]\n",
    "                        predicted_answer = actual_answer  # Use context's exact case\n",
    "                        print(f\"Training format case-corrected: '{pred.get('answer_text', '')}' '{actual_answer}'\")\n",
    "                    else:\n",
    "                        # Try fuzzy matching\n",
    "                        import re\n",
    "                        clean_answer = re.sub(r'[^\\w\\s]', '', predicted_answer.strip())\n",
    "                        pattern = re.escape(clean_answer).replace(r'\\ ', r'\\s+')\n",
    "                        \n",
    "                        match = re.search(pattern, context, re.IGNORECASE)\n",
    "                        if match:\n",
    "                            answer_start = match.start()\n",
    "                            predicted_answer = context[match.start():match.end()]\n",
    "                            print(f\"Training format fuzzy-matched: '{pred.get('answer_text', '')}' '{predicted_answer}'\")\n",
    "                        else:\n",
    "                            print(f\"Warning: Answer '{predicted_answer}' not found in context, treating as no-answer\")\n",
    "                            # Convert to no-answer case\n",
    "                            training_sample = {\n",
    "                                'id': pred['id'],\n",
    "                                'question': pred['question'],\n",
    "                                'context': context,\n",
    "                                'answers': {\n",
    "                                    'text': [],\n",
    "                                    'answer_start': []\n",
    "                                },\n",
    "                                'is_impossible': True,\n",
    "                                'is_pseudo_labeled': True,\n",
    "                                'confidence': pred['confidence'],\n",
    "                                'human_reviewed': pred.get('human_corrected', False),\n",
    "                                'question_type': question_type\n",
    "                            }\n",
    "                            training_samples.append(training_sample)\n",
    "                            continue\n",
    "            \n",
    "            # Validate the answer position\n",
    "            if answer_start >= 0 and answer_start + len(predicted_answer) <= len(context):\n",
    "                text_at_position = context[answer_start:answer_start + len(predicted_answer)]\n",
    "                if text_at_position != predicted_answer:\n",
    "                    print(f\"Position mismatch for {pred['id']}: expected '{predicted_answer}' but found '{text_at_position}' at position {answer_start}\")\n",
    "                    predicted_answer = text_at_position\n",
    "            \n",
    "            training_sample = {\n",
    "                'id': pred['id'],\n",
    "                'question': pred['question'],\n",
    "                'context': context,\n",
    "                'answers': {\n",
    "                    'text': [predicted_answer],\n",
    "                    'answer_start': [max(0, answer_start)]  # Ensure non-negative\n",
    "                },\n",
    "                'is_impossible': False,\n",
    "                'is_pseudo_labeled': True,\n",
    "                'confidence': pred['confidence'],\n",
    "                'human_reviewed': pred.get('human_corrected', False),\n",
    "                'question_type': question_type  # Add question type\n",
    "            }\n",
    "        \n",
    "        training_samples.append(training_sample)\n",
    "    \n",
    "    # Print final distribution for verification\n",
    "    distribution = defaultdict(int)\n",
    "    for sample in training_samples:\n",
    "        distribution[sample['question_type']] += 1\n",
    "    \n",
    "    print(f\"Training format distribution: {dict(distribution)}\")\n",
    "    \n",
    "    return training_samples\n",
    "\n",
    "def display_review_statistics(review_file=\"reviewed_samples.json\"):\n",
    "    \"\"\"Display statistics from all reviews with better error handling\"\"\"\n",
    "    if not os.path.exists(review_file):\n",
    "        print(\"No review file found\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(review_file, 'r') as f:\n",
    "            reviews = json.load(f)\n",
    "        \n",
    "        print(f\"\\nCUMULATIVE REVIEW STATISTICS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Total entries in file: {len(reviews)}\")\n",
    "        \n",
    "        # Filter out control entries (skip, quit, etc.)\n",
    "        actual_reviews = {k: v for k, v in reviews.items() \n",
    "                         if v.get('your_decision') not in ['skipped_remaining', 'quit']}\n",
    "        \n",
    "        print(f\"Actual sample reviews: {len(actual_reviews)}\")\n",
    "        \n",
    "        if len(actual_reviews) > 0:\n",
    "            decisions = [review['your_decision'] for review in actual_reviews.values()]\n",
    "            print(f\"Accepted: {decisions.count('accepted')}\")\n",
    "            print(f\"Corrected: {decisions.count('corrected')}\")\n",
    "            print(f\"Rejected: {decisions.count('rejected')}\")\n",
    "            \n",
    "            # Group by iteration\n",
    "            iterations = {}\n",
    "            for review in actual_reviews.values():\n",
    "                iter_num = review.get('iteration', 0)\n",
    "                if iter_num not in iterations:\n",
    "                    iterations[iter_num] = []\n",
    "                iterations[iter_num].append(review['your_decision'])\n",
    "            \n",
    "            print(\"\\nBy Iteration:\")\n",
    "            for iter_num in sorted(iterations.keys()):\n",
    "                decisions = iterations[iter_num]\n",
    "                print(f\"Iteration {iter_num}: {len(decisions)} samples\")\n",
    "                print(f\"Accepted: {decisions.count('accepted')}\")\n",
    "                print(f\"Corrected: {decisions.count('corrected')}\")\n",
    "                print(f\"Rejected: {decisions.count('rejected')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading review statistics: {e}\")\n",
    "\n",
    "def split_approved_samples(approved_samples, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Stratified split that maintains question type balance across train/val/test\n",
    "    \"\"\"\n",
    "    if len(approved_samples) == 0:\n",
    "        return [], [], []\n",
    "    \n",
    "    print(f\"Performing stratified split of {len(approved_samples)} samples...\")\n",
    "    \n",
    "    # Group samples by question type\n",
    "    by_question_type = defaultdict(list)\n",
    "    for sample in approved_samples:\n",
    "        question_type = get_question_type(sample['question'])\n",
    "        by_question_type[question_type].append(sample)\n",
    "    \n",
    "    print(\"Input distribution:\")\n",
    "    for q_type, samples in by_question_type.items():\n",
    "        print(f\"{q_type}: {len(samples)} samples\")\n",
    "    \n",
    "    # Perform stratified split for each question type\n",
    "    train_split = []\n",
    "    val_split = []\n",
    "    test_split = []\n",
    "    \n",
    "    random.seed(42)  # For reproducible splits\n",
    "    \n",
    "    for question_type, samples in by_question_type.items():\n",
    "        if len(samples) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Shuffle samples within this question type\n",
    "        samples_copy = samples.copy()\n",
    "        random.shuffle(samples_copy)\n",
    "        \n",
    "        n = len(samples_copy)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "        \n",
    "        # Split this question type\n",
    "        type_train = samples_copy[:train_end]\n",
    "        type_val = samples_copy[train_end:val_end]\n",
    "        type_test = samples_copy[val_end:]\n",
    "        \n",
    "        # Add to overall splits\n",
    "        train_split.extend(type_train)\n",
    "        val_split.extend(type_val)\n",
    "        test_split.extend(type_test)\n",
    "        \n",
    "        print(f\"{question_type}: {len(type_train)} train + {len(type_val)} val + {len(type_test)} test\")\n",
    "    \n",
    "    # Final shuffle to mix question types within each split\n",
    "    random.shuffle(train_split)\n",
    "    random.shuffle(val_split)\n",
    "    random.shuffle(test_split)\n",
    "    \n",
    "    print(f\"Stratified split result: {len(train_split)} train + {len(val_split)} val + {len(test_split)} test\")\n",
    "    \n",
    "    # Verify balance is maintained\n",
    "    print(\"Final distribution verification:\")\n",
    "    for split_name, split_data in [(\"Train\", train_split), (\"Val\", val_split), (\"Test\", test_split)]:\n",
    "        if len(split_data) > 0:\n",
    "            split_distribution = defaultdict(int)\n",
    "            for sample in split_data:\n",
    "                q_type = get_question_type(sample['question'])\n",
    "                split_distribution[q_type] += 1\n",
    "            \n",
    "            print(f\"{split_name}: {dict(split_distribution)}\")\n",
    "    \n",
    "    return train_split, val_split, test_split\n",
    "\n",
    "def check_answer_indices(qa_samples, num_samples=10):\n",
    "    \"\"\"\n",
    "    Simple function to check if answer indices are correct\n",
    "    Just add this anywhere in your script and call it\n",
    "    \"\"\"\n",
    "    print(\"CHECKING ANSWER INDICES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check first N samples or all if fewer\n",
    "    samples_to_check = qa_samples[:num_samples] if len(qa_samples) > num_samples else qa_samples\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_checked = 0\n",
    "    \n",
    "    for i, sample in enumerate(samples_to_check):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        \n",
    "        context = sample.get('context', '')\n",
    "        answer_text = sample.get('answer_text', '')\n",
    "        answer_start = sample.get('answer_start', -1)\n",
    "        sample_id = sample.get('id', f'sample_{i}')\n",
    "        \n",
    "        print(f\"ID: {sample_id}\")\n",
    "        print(f\"Question: {sample['question']}\")\n",
    "        print(f\"Expected Answer: '{answer_text}'\")\n",
    "        print(f\"Answer Start Index: {answer_start}\")\n",
    "        \n",
    "        # Skip empty answers\n",
    "        if not answer_text or answer_text.strip() == '':\n",
    "            print(\"Empty answer (OK)\")\n",
    "            correct_count += 1\n",
    "            total_checked += 1\n",
    "            continue\n",
    "        \n",
    "        # Check if index is valid\n",
    "        if answer_start < 0 or answer_start >= len(context):\n",
    "            print(f\"INVALID INDEX: {answer_start} (context length: {len(context)})\")\n",
    "            total_checked += 1\n",
    "            continue\n",
    "        \n",
    "        # Extract text at the index\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        extracted_text = context[answer_start:answer_end]\n",
    "        \n",
    "        print(f\"Text at Index {answer_start}-{answer_end}: '{extracted_text}'\")\n",
    "        \n",
    "        # Show context around the answer\n",
    "        start_preview = max(0, answer_start - 20)\n",
    "        end_preview = min(len(context), answer_end + 20)\n",
    "        context_preview = context[start_preview:end_preview]\n",
    "        highlighted = context_preview.replace(extracted_text, f\">>>{extracted_text}<<<\")\n",
    "        print(f\"Context: ...{highlighted}...\")\n",
    "        \n",
    "        # Check if they match\n",
    "        if extracted_text == answer_text:\n",
    "            print(\"CORRECT INDEX\")\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            print(\"WRONG INDEX\")\n",
    "            # Try to find where it should be\n",
    "            correct_index = context.find(answer_text)\n",
    "            if correct_index != -1:\n",
    "                print(f\"Should be at index: {correct_index}\")\n",
    "            else:\n",
    "                print(f\"Answer text not found in context!\")\n",
    "        \n",
    "        total_checked += 1\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"Checked: {total_checked} samples\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Wrong: {total_checked - correct_count}\")\n",
    "    print(f\"Accuracy: {correct_count/total_checked*100:.1f}%\")\n",
    "    \n",
    "    return correct_count, total_checked\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import contextlib\n",
    "\n",
    "# Class for capturing output\n",
    "class OutputLogger:\n",
    "    \"\"\"Captures both console output and saves to file\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, 'w', encoding='utf-8')\n",
    "        \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)  # Show on console\n",
    "        self.log.write(message)       # Also save to file\n",
    "        self.log.flush()              # Ensure immediate writing\n",
    "        \n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "        \n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "def reset_reused_flags(review_file=\"reviewed_samples.json\"):\n",
    "    \"\"\"Reset all reused flags to False and remove last_used_iteration for fresh training start\"\"\"\n",
    "    if not os.path.exists(review_file):\n",
    "        print(\"No review file found to reset\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(review_file, 'r') as f:\n",
    "            reviews = json.load(f)\n",
    "        \n",
    "        reset_count = 0\n",
    "        removed_last_used_count = 0\n",
    "        \n",
    "        for review_key, review_data in reviews.items():\n",
    "            # Reset reused flag if it was True\n",
    "            if review_data.get('reused', False) == True:\n",
    "                reviews[review_key]['reused'] = False\n",
    "                reset_count += 1\n",
    "            \n",
    "            # Remove last_used_iteration field if it exists (from previous runs)\n",
    "            if 'last_used_iteration' in review_data:\n",
    "                del reviews[review_key]['last_used_iteration']\n",
    "                removed_last_used_count += 1\n",
    "        \n",
    "        with open(review_file, 'w') as f:\n",
    "            json.dump(reviews, f, indent=2)\n",
    "        \n",
    "        print(f\"Reset {reset_count} samples to reused=False\")\n",
    "        print(f\"Removed {removed_last_used_count} last_used_iteration fields from previous runs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting reused flags: {e}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main self-training pipeline using real annotated data for initial training\"\"\"\n",
    "    \n",
    "    # Create output logger with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"training_output_{timestamp}.log\"\n",
    "    \n",
    "    # Redirect stdout to capture all output\n",
    "    output_logger = OutputLogger(output_filename)\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = output_logger\n",
    "    \n",
    "    try:\n",
    "        print(\"SafetyBERT Self-Training QA System - Real Annotated Data Version\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Output being saved to: {output_filename}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # NEW: Reset all reused flags for fresh training start\n",
    "        print(\"\\nResetting all reused flags for fresh training...\")\n",
    "        reset_reused_flags()\n",
    "        \n",
    "        # Phase 1: Load real annotated data and train base model\n",
    "        print(\"\\nPHASE 1: Real Data Loading & Base Model Training\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Load your annotated data\n",
    "        annotated_data = load_annotated_data(\"annotated-data.csv\")\n",
    "        \n",
    "        if not annotated_data:\n",
    "            print(\"Failed to load annotated data\")\n",
    "            return\n",
    "        \n",
    "        # Convert to SQuAD format with your questions\n",
    "        squad_data = convert_annotated_to_squad_format(annotated_data)\n",
    "        \n",
    "        if not squad_data:\n",
    "            print(\"Failed to convert annotated data to QA format\")\n",
    "            return\n",
    "        \n",
    "        # Split annotated data for training\n",
    "        train_data, val_data, test_data = create_train_val_test_split(squad_data, 0.7, 0.15, 0.15)\n",
    "        \n",
    "        # Setup model\n",
    "        model, tokenizer = setup_model_and_tokenizer(\"/home/exouser/QA-traninig-self-learning-MSHA/safetyBERT/\")\n",
    "        if model is None or tokenizer is None:\n",
    "            return\n",
    "        \n",
    "        # Preprocess data\n",
    "        train_dataset = preprocess_data(train_data, tokenizer)\n",
    "        val_dataset = preprocess_data(val_data, tokenizer)\n",
    "        \n",
    "        # Train base model on real annotated data\n",
    "        trainer = create_trainer(model, tokenizer, train_dataset, val_dataset)\n",
    "        \n",
    "        print(\"\\nTraining base model on real annotated data...\")\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Save base model\n",
    "        model.save_pretrained(\"./base_safety_qa_model_real_data\")\n",
    "        tokenizer.save_pretrained(\"./base_safety_qa_model_real_data\")\n",
    "        print(\"Base model saved to ./base_safety_qa_model_real_data\")\n",
    "        \n",
    "        # Evaluate base model on real test set\n",
    "        base_score = evaluate_model_qa(model, tokenizer, test_data, \"Base Model - Real Annotated Test Data\")\n",
    "        \n",
    "        # Phase 2: Load prediction pool (external accident narratives for self-training)\n",
    "        print(\"\\nPHASE 2: Loading Prediction Pool for Self-Training\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Load external narratives for self-training (different from training data)\n",
    "        all_accident_narratives = load_accident_narratives(file_path=\"data-mill-without-annotated-samples-and-fatality-occupational-illness.csv\")\n",
    "        if not all_accident_narratives:\n",
    "            print(\"No external narratives available for self-training\")\n",
    "            return\n",
    "        \n",
    "        # Generate synthetic QA pairs for external narratives using question templates\n",
    "        all_synthetic_qa_pairs = generate_questions_for_narratives(all_accident_narratives)\n",
    "        print(f\"Generated {len(all_synthetic_qa_pairs)} total QA pairs from {len(all_accident_narratives)} external narratives\")\n",
    "        \n",
    "        # Phase 3: Self-training loop\n",
    "        print(\"\\nPHASE 3: Self-Training Loop\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Generate predictions for ALL QA pairs ONCE\n",
    "        print(\"Generating predictions for all QA pairs (one-time processing)...\")\n",
    "        all_predictions = predict_with_confidence(model, tokenizer, all_synthetic_qa_pairs)   \n",
    "        \n",
    "        # Sort by confidence (lowest first) for efficient sampling\n",
    "        all_predictions.sort(key=lambda x: x['confidence'], reverse=False)\n",
    "        print(f\"Sorted {len(all_predictions)} predictions by confidence\")\n",
    "        print(f\"Confidence range: {all_predictions[-1]['confidence']:.3f} - {all_predictions[0]['confidence']:.3f}\")\n",
    "        \n",
    "        # Initialize datasets - START WITH ORIGINAL ANNOTATED DATA AS FOUNDATION\n",
    "        current_training_data = train_data.copy()      \n",
    "        current_validation_data = val_data.copy()      \n",
    "        current_test_data = test_data.copy()       \n",
    "        \n",
    "        best_score = base_score\n",
    "        max_iterations = 20\n",
    "        used_prediction_indices = set()  # Track which predictions we've already used\n",
    "        \n",
    "        performance_history = [{'iteration': 0,\n",
    "                                'score': base_score,\n",
    "                                'training_size': len(current_training_data),\n",
    "                                'validation_size': len(current_validation_data),    \n",
    "                                'test_size': len(current_test_data),\n",
    "                                'approved_samples': 0 }]\n",
    "        \n",
    "        # User-configurable parameters\n",
    "        SAMPLES_PER_ITERATION = 200  \n",
    "        \n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"Samples per iteration: {SAMPLES_PER_ITERATION}\")\n",
    "        print(f\"Max iterations: {max_iterations}\")\n",
    "\n",
    "        early_stopped = False\n",
    "        best_score_iteration = 0\n",
    "        \n",
    "        for iteration in range(1, max_iterations + 1):\n",
    "            print(f\"\\nITERATION {iteration}\")\n",
    "            print(\"=\" * 30)\n",
    "            \n",
    "            if iteration == 1:\n",
    "                print('Using base model predictions for iteration 1')\n",
    "                current_predictions = all_predictions\n",
    "                # For iteration 1, predictions don't have original_index, so add them\n",
    "                for i, pred in enumerate(current_predictions):\n",
    "                    pred['original_index'] = i\n",
    "            else:\n",
    "                # Generate fresh predictions with updated model\n",
    "                print(f'Generating fresh predictions with iteration {iteration} model...')\n",
    "\n",
    "                unused_qa_pairs = []\n",
    "                unused_original_indices = []\n",
    "\n",
    "                for i, qa in enumerate(all_synthetic_qa_pairs):\n",
    "                    if i not in used_prediction_indices:\n",
    "                        unused_qa_pairs.append(qa)\n",
    "                        unused_original_indices.append(i)\n",
    "\n",
    "                if len(unused_qa_pairs) == 0:\n",
    "                    print(\"All QA pairs used - stopping training\")\n",
    "                    break\n",
    "                            \n",
    "                # Generate predictions with CURRENT (improved) model\n",
    "                fresh_predictions  = predict_with_confidence(model, tokenizer, unused_qa_pairs)\n",
    "                current_predictions = []\n",
    "                for j, pred in enumerate(fresh_predictions):\n",
    "                    pred['original_index'] = unused_original_indices[j]  # Add original index\n",
    "                    current_predictions.append(pred)\n",
    "                                \n",
    "                current_predictions.sort(key=lambda x: x['confidence'], reverse=False)\n",
    "                \n",
    "                print(f'Generated {len(current_predictions)} fresh predictions')\n",
    "                if current_predictions:\n",
    "                    confidences = [pred.get('confidence', -1) for pred in current_predictions]\n",
    "                    print(f'Confidence range: {min(confidences):.3f} - {max(confidences):.3f}')\n",
    "                    print(f'Average confidence: {sum(confidences)/len(confidences):.3f}')\n",
    "            \n",
    "            # Select low-confidence samples for this iteration\n",
    "            hybrid_samples = select_hybrid_samples(\n",
    "                current_predictions, \n",
    "                used_prediction_indices,\n",
    "                max_samples=SAMPLES_PER_ITERATION\n",
    "            )\n",
    "                  \n",
    "            if len(hybrid_samples) == 0:\n",
    "                print(\"No samples found for hybrid selection. Stopping self-training.\")\n",
    "                break\n",
    "\n",
    "            print(f\"HUMAN REVIEW: {len(hybrid_samples)} selected samples (hybrid strategy)\")\n",
    "            \n",
    "            # HUMAN REVIEW\n",
    "            reviewed_samples, all_processed_indices = interactive_review(hybrid_samples, iteration, all_synthetic_qa_pairs)\n",
    "\n",
    "            # Add ALL processed indices (both reused and newly reviewed) to used_prediction_indices\n",
    "            for idx in all_processed_indices:\n",
    "                used_prediction_indices.add(idx)\n",
    "\n",
    "            print(f\"Added {len(all_processed_indices)} processed samples to used_prediction_indices\")\n",
    "            print(f\"Total used samples: {len(used_prediction_indices)}/{len(all_synthetic_qa_pairs)}\")\n",
    "            \n",
    "            approved_samples = convert_reviewed_samples_to_training_format(reviewed_samples)\n",
    "\n",
    "            print(f\"\\nChecking approved samples BEFORE conversion...\")\n",
    "            samples_with_answers = [s for s in approved_samples if s.get('answer_text', '').strip() != '']\n",
    "            print(f\"Found {len(samples_with_answers)} samples with actual answers out of {len(approved_samples)} total\")\n",
    "            \n",
    "            if len(approved_samples) == 0:\n",
    "                print(\"No samples approved by human review. Stopping self-training.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Human approved: {len(approved_samples)}/{len(hybrid_samples)} samples\")\n",
    "            print(f'Total used QA pairs: {len(used_prediction_indices)}/{len(all_synthetic_qa_pairs)}')\n",
    "\n",
    "            # **SPLIT APPROVED SAMPLES 70:15:15**\n",
    "            approved_train, approved_val, approved_test = split_approved_samples(approved_samples)\n",
    "            \n",
    "            if len(approved_train) == 0 and len(approved_val) == 0 and len(approved_test) == 0:\n",
    "                print(\"No samples to add after splitting - continuing...\")\n",
    "                continue\n",
    "            \n",
    "            # Convert each split to training format\n",
    "            pseudo_train_data = convert_to_training_format(approved_train)\n",
    "            pseudo_val_data = convert_to_training_format(approved_val)\n",
    "            pseudo_test_data = convert_to_training_format(approved_test)\n",
    "            \n",
    "            # Add to respective datasets (FOUNDATION + NEW SAMPLES)\n",
    "            current_training_data.extend(pseudo_train_data)\n",
    "            current_validation_data.extend(pseudo_val_data)\n",
    "            current_test_data.extend(pseudo_test_data)\n",
    "\n",
    "            # Update reused flags for all approved samples (so they won't be selected again)\n",
    "            print(f\"Updating reused status for all approved samples...\")\n",
    "            try:\n",
    "                # Load existing reviews\n",
    "                if os.path.exists(\"reviewed_samples.json\"):\n",
    "                    with open(\"reviewed_samples.json\", 'r') as f:\n",
    "                        existing_reviews = json.load(f)\n",
    "                else:\n",
    "                    existing_reviews = {}\n",
    "                \n",
    "                # Mark ALL approved samples as reused (so they won't be selected again)\n",
    "                marked_count = 0\n",
    "                for sample in approved_samples:\n",
    "                    sample_key = create_sample_key(sample['context'], sample['question'])\n",
    "                    \n",
    "                    # Find and update the matching review entry\n",
    "                    for review_key, review_data in existing_reviews.items():\n",
    "                        existing_sample_key = create_sample_key(review_data['narrative'], review_data['question'])\n",
    "                        if existing_sample_key == sample_key:\n",
    "                            # Update reused flag for approved samples\n",
    "                            decision = review_data.get('your_decision', '')\n",
    "                            if decision in ['accepted', 'corrected']:\n",
    "                                existing_reviews[review_key]['reused'] = True\n",
    "                                existing_reviews[review_key]['last_used_iteration'] = iteration\n",
    "                                marked_count += 1\n",
    "                            break\n",
    "                \n",
    "                # Save updated reviews\n",
    "                with open(\"reviewed_samples.json\", 'w') as f:\n",
    "                    json.dump(existing_reviews, f, indent=2)\n",
    "                \n",
    "                print(f\"Marked {marked_count} approved samples as reused=True\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error marking samples as reused: {e}\")\n",
    "            \n",
    "            print(f\"Updated dataset sizes:\")\n",
    "            print(f\"Training: {len(current_training_data)} (+{len(pseudo_train_data)} new)\")\n",
    "            print(f\"Validation: {len(current_validation_data)} (+{len(pseudo_val_data)} new)\")\n",
    "            print(f\"Test: {len(current_test_data)} (+{len(pseudo_test_data)} new)\")\n",
    "\n",
    "            # Retrain model with expanded datasets\n",
    "            train_dataset_expanded = preprocess_data(current_training_data, tokenizer)\n",
    "            val_dataset_expanded = preprocess_data(current_validation_data, tokenizer)\n",
    "\n",
    "            print(f\"Loading fresh BERT model for iteration {iteration}\")\n",
    "            model, tokenizer = setup_model_and_tokenizer(\"/home/exouser/QA-traninig-self-learning-MSHA/safetyBERT/\")\n",
    "            \n",
    "            trainer_expanded = create_trainer(model, tokenizer, train_dataset_expanded, val_dataset_expanded)\n",
    "            print(f\"Training from scratch on {len(current_training_data)} total samples\")\n",
    "            train_result = trainer_expanded.train()\n",
    "            \n",
    "            # Evaluate improved model on current validation set\n",
    "            current_score = evaluate_model_qa(model, tokenizer, current_validation_data, f\"Iteration {iteration} - Current Validation\", print_wrong_predictions=False)\n",
    "            \n",
    "            # Track performance\n",
    "            performance_history.append({\n",
    "                'iteration': iteration, \n",
    "                'score': current_score, \n",
    "                'training_size': len(current_training_data),\n",
    "                'validation_size': len(current_validation_data),\n",
    "                'test_size': len(current_test_data),\n",
    "                'approved_samples': len(approved_samples)\n",
    "            })\n",
    "            \n",
    "            print(f\"Iteration {iteration} Score: {current_score:.1f}% (vs Base: {base_score:.1f}%)\")\n",
    "\n",
    "            # Early stopping check\n",
    "            SL_EARLY_STOPPING_PATIENCE = 3  # Allow 3 declining iterations\n",
    "\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_score_iteration = iteration  # Update when best was achieved\n",
    "                print(f\"New best score: {best_score:.1f}% at iteration {iteration}\")\n",
    "                \n",
    "                # Save improved model\n",
    "                model.save_pretrained(f\"./self_trained_model_iter_{iteration}\")\n",
    "                tokenizer.save_pretrained(f\"./self_trained_model_iter_{iteration}\")\n",
    "                print(f\"Best model saved to ./best_model_iter_{iteration}\")\n",
    "                \n",
    "            iterations_since_best = iteration - best_score_iteration\n",
    "            \n",
    "            if iterations_since_best >= SL_EARLY_STOPPING_PATIENCE:\n",
    "                print(f'No improvement for {SL_EARLY_STOPPING_PATIENCE} iterations since best score!')\n",
    "                print(f'Best score: {best_score:.1f}% (iteration {best_score_iteration})')\n",
    "                print(f'Current score: {current_score:.1f}% (iteration {iteration})')\n",
    "\n",
    "                # Save the final model before stopping\n",
    "                print(f\"Saving final iteration model before early stopping...\")\n",
    "                model.save_pretrained(f\"./final_model_iter_{iteration}\")\n",
    "                tokenizer.save_pretrained(f\"./final_model_iter_{iteration}\")\n",
    "                print(f\"Final model saved to ./final_model_iter_{iteration}\")\n",
    "\n",
    "                early_stopped = True\n",
    "\n",
    "                print(f'Stopping early...')\n",
    "                break\n",
    "\n",
    "            print(f\"Iterations since best: {iterations_since_best}/{SL_EARLY_STOPPING_PATIENCE}\")\n",
    "                \n",
    "            if current_score < best_score - 70:\n",
    "\n",
    "                early_stopped = True\n",
    "            \n",
    "                print(\"Significant performance degradation. Stopping self-training.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Continuing training...\")\n",
    "\n",
    "        if not early_stopped:\n",
    "            print(f\"Maximum iterations ({max_iterations}) reached.\")\n",
    "            print(f\"Saving final iteration model...\")\n",
    "            model.save_pretrained(f\"./final_model_iter_{iteration}\")\n",
    "            tokenizer.save_pretrained(f\"./final_model_iter_{iteration}\")\n",
    "            print(f\"Final model saved to ./final_model_iter_{iteration}\")\n",
    "\n",
    "        # Final evaluation summary\n",
    "        print(\"\\nFINAL RESULTS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(\"Performance History:\")\n",
    "        for hist in performance_history:\n",
    "            approved = hist.get('approved_samples', 'N/A')\n",
    "            print(f\"Iteration {hist['iteration']}: {hist['score']:.1f}% \"\n",
    "                  f\"(Train: {hist['training_size']}, Val: {hist.get('validation_size', 'N/A')}, \"\n",
    "                  f\"Test: {hist.get('test_size', 'N/A')}, Approved: {approved})\")\n",
    "        \n",
    "        improvement = best_score - base_score\n",
    "        print(f\"\\nFinal Improvement: {improvement:+.1f}% (Base: {base_score:.1f}% ? Best: {best_score:.1f}%)\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(\"Self-training successful!\")\n",
    "        else:\n",
    "            print(\"Self-training did not improve performance\")\n",
    "        \n",
    "        # Final evaluation on accumulated test data\n",
    "        print(\"\\nFINAL EVALUATION ON ACCUMULATED TEST DATA\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Load the best saved model for final evaluation\n",
    "        if best_score > base_score:  # If we found any improvements\n",
    "            import glob\n",
    "            model_dirs = glob.glob(\"./self_trained_model_iter_*\")\n",
    "            if model_dirs:\n",
    "                # Find the model with the highest iteration number (most recent best)\n",
    "                latest_best_model = max(model_dirs, key=lambda x: int(x.split('_')[-1]))\n",
    "                print(f\"Loading best model from {latest_best_model} for final evaluation...\")\n",
    "                model = BertForQuestionAnswering.from_pretrained(latest_best_model)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(latest_best_model)\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                model.to(device)\n",
    "                print(f\"Best model loaded (best score: {best_score:.1f}%)\")\n",
    "            else:\n",
    "                print(\"No saved iteration models found, using current model\")\n",
    "        else:\n",
    "            print(\"No improvements found, using base model for final evaluation\")\n",
    "     \n",
    "        print(f\"Evaluating on {len(current_test_data)} accumulated test samples...\")\n",
    "        final_test_score = evaluate_model_qa(model, tokenizer, current_test_data, \"Final Accumulated Test Set\", print_wrong_predictions=True)\n",
    "        \n",
    "        print(f\"Final Test Score: {final_test_score:.1f}%\")\n",
    "\n",
    "        print(f\"\\nSAVING DETAILED TEST RESULTS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Generating detailed predictions for {len(current_test_data)} test samples...\")\n",
    "            \n",
    "            detailed_test_results = []\n",
    "            model.eval()\n",
    "            device = next(model.parameters()).device\n",
    "            \n",
    "            for i, test_sample in enumerate(current_test_data):\n",
    "                # Get model prediction\n",
    "                inputs = tokenizer(\n",
    "                    test_sample[\"question\"],\n",
    "                    test_sample[\"context\"], \n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=128,\n",
    "                    truncation=True,\n",
    "                    padding=True\n",
    "                )\n",
    "                \n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                start_idx = torch.argmax(outputs.start_logits[0]).item()\n",
    "                end_idx = torch.argmax(outputs.end_logits[0]).item()\n",
    "                \n",
    "                if start_idx <= end_idx and start_idx < len(inputs[\"input_ids\"][0]) and end_idx < len(inputs[\"input_ids\"][0]):\n",
    "                    predicted_answer = tokenizer.decode(\n",
    "                        inputs[\"input_ids\"][0][start_idx:end_idx+1],\n",
    "                        skip_special_tokens=True\n",
    "                    ).strip()\n",
    "                else:\n",
    "                    predicted_answer = \"\"\n",
    "                \n",
    "                # Get actual answer(s)\n",
    "                if test_sample[\"answers\"][\"text\"]:\n",
    "                    actual_answers = test_sample[\"answers\"][\"text\"] if isinstance(test_sample[\"answers\"][\"text\"], list) else [test_sample[\"answers\"][\"text\"]]\n",
    "                    actual_answer = actual_answers[0]  # Use first answer as primary\n",
    "                    all_actual_answers = actual_answers\n",
    "                else:\n",
    "                    actual_answer = \"\"\n",
    "                    all_actual_answers = []\n",
    "                \n",
    "                # Determine question type\n",
    "                question_type = get_question_type(test_sample['question'])\n",
    "                \n",
    "                # Calculate if prediction is correct (exact match)\n",
    "                is_correct = any(\n",
    "                    predicted_answer.lower().strip() == ans.lower().strip() \n",
    "                    for ans in all_actual_answers\n",
    "                ) if all_actual_answers else (predicted_answer.strip() == \"\")\n",
    "                \n",
    "                # Create detailed result entry\n",
    "                result_entry = {\n",
    "                    \"sample_id\": test_sample.get(\"id\", f\"test_sample_{i}\"),\n",
    "                    \"question_type\": question_type,\n",
    "                    \"narrative\": test_sample[\"context\"],\n",
    "                    \"question\": test_sample[\"question\"],\n",
    "                    \"actual_answer\": actual_answer,\n",
    "                    \"all_actual_answers\": all_actual_answers,\n",
    "                    \"predicted_answer\": predicted_answer,\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"is_impossible\": test_sample.get(\"is_impossible\", False),\n",
    "                    \"confidence_scores\": {\n",
    "                        \"start_logit_max\": float(torch.max(outputs.start_logits[0]).item()),\n",
    "                        \"end_logit_max\": float(torch.max(outputs.end_logits[0]).item())\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                detailed_test_results.append(result_entry)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Processed {i + 1}/{len(current_test_data)} samples...\")\n",
    "            \n",
    "            # Save detailed results\n",
    "            with open(\"detailed_test_results.json\", \"w\") as f:\n",
    "                json.dump(detailed_test_results, f, indent=2)\n",
    "            print(\"Saved: detailed_test_results.json\")\n",
    "            \n",
    "            # Generate summary statistics\n",
    "            total_samples = len(detailed_test_results)\n",
    "            correct_predictions = sum(1 for r in detailed_test_results if r[\"is_correct\"])\n",
    "            accuracy = (correct_predictions / total_samples * 100) if total_samples > 0 else 0\n",
    "            \n",
    "            # By question type\n",
    "            by_question_type = {}\n",
    "            for result in detailed_test_results:\n",
    "                q_type = result[\"question_type\"]\n",
    "                if q_type not in by_question_type:\n",
    "                    by_question_type[q_type] = {\"total\": 0, \"correct\": 0}\n",
    "                by_question_type[q_type][\"total\"] += 1\n",
    "                if result[\"is_correct\"]:\n",
    "                    by_question_type[q_type][\"correct\"] += 1\n",
    "            \n",
    "            # Save summary\n",
    "            summary = {\n",
    "                \"total_test_samples\": total_samples,\n",
    "                \"correct_predictions\": correct_predictions,\n",
    "                \"overall_accuracy\": accuracy,\n",
    "                \"final_test_score\": final_test_score,\n",
    "                \"by_question_type\": {\n",
    "                    q_type: {\n",
    "                        \"total\": stats[\"total\"],\n",
    "                        \"correct\": stats[\"correct\"],\n",
    "                        \"accuracy\": (stats[\"correct\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "                    }\n",
    "                    for q_type, stats in by_question_type.items()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(\"test_results_summary.json\", \"w\") as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            print(\"Saved: test_results_summary.json\")\n",
    "            \n",
    "            print(f\"\\nTest Results Summary:\")\n",
    "            print(f\"Total samples: {total_samples}\")\n",
    "            print(f\"Correct predictions: {correct_predictions}\")\n",
    "            print(f\"Overall accuracy: {accuracy:.1f}%\")\n",
    "            print(f\"F1 score: {final_test_score:.1f}%\")\n",
    "            \n",
    "            print(f\"\\nBy Question Type:\")\n",
    "            for q_type, stats in by_question_type.items():\n",
    "                acc = (stats[\"correct\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "                print(f\"{q_type}: {stats['correct']}/{stats['total']} ({acc:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating detailed test results: {e}\")\n",
    "        \n",
    "        # Save final accumulated datasets in SQuAD v2.0 format\n",
    "        print(f\"\\nSAVING FINAL ACCUMULATED DATASETS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Save training data\n",
    "            print(f\"Saving training data: {len(current_training_data)} samples...\")\n",
    "            with open(\"final_train_data.json\", \"w\") as f:\n",
    "                json.dump(current_training_data, f, indent=2)\n",
    "            print(\"Saved: final_train_data.json\")\n",
    "            \n",
    "            # Save validation data\n",
    "            print(f\"Saving validation data: {len(current_validation_data)} samples...\")\n",
    "            with open(\"final_val_data.json\", \"w\") as f:\n",
    "                json.dump(current_validation_data, f, indent=2)\n",
    "            print(\"Saved: final_val_data.json\")\n",
    "            \n",
    "            # Save test data\n",
    "            print(f\"Saving test data: {len(current_test_data)} samples...\")\n",
    "            with open(\"final_test_data.json\", \"w\") as f:\n",
    "                json.dump(current_test_data, f, indent=2)\n",
    "            print(\"Saved: final_test_data.json\")\n",
    "            \n",
    "            # Save summary statistics\n",
    "            dataset_summary = {\n",
    "                \"training_started\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"total_iterations\": len(performance_history) - 1,  # -1 because includes base (iteration 0)\n",
    "                \"final_training_size\": len(current_training_data),\n",
    "                \"final_validation_size\": len(current_validation_data),\n",
    "                \"final_test_size\": len(current_test_data),\n",
    "                \"total_samples\": len(current_training_data) + len(current_validation_data) + len(current_test_data),\n",
    "                \"base_score\": base_score,\n",
    "                \"best_score\": best_score,\n",
    "                \"final_test_score\": final_test_score,\n",
    "                \"final_improvement\": best_score - base_score,\n",
    "                \"used_predictions\": len(used_prediction_indices),\n",
    "                \"total_available_predictions\": len(all_synthetic_qa_pairs),\n",
    "                \"utilization_rate\": len(used_prediction_indices) / len(all_synthetic_qa_pairs) * 100,\n",
    "                \"performance_history\": performance_history\n",
    "            }\n",
    "            \n",
    "            with open(\"final_dataset_summary.json\", \"w\") as f:\n",
    "                json.dump(dataset_summary, f, indent=2)\n",
    "            print(\"? Saved: final_dataset_summary.json\")\n",
    "            \n",
    "            print(f\"\\nFinal Dataset Summary:\")\n",
    "            print(f\"Training: {len(current_training_data)} samples\")\n",
    "            print(f\"Validation: {len(current_validation_data)} samples\") \n",
    "            print(f\"Test: {len(current_test_data)} samples\")\n",
    "            print(f\"Total: {len(current_training_data) + len(current_validation_data) + len(current_test_data)} samples\")\n",
    "            print(f\"Used {len(used_prediction_indices)}/{len(all_synthetic_qa_pairs)} available predictions ({len(used_prediction_indices)/len(all_synthetic_qa_pairs)*100:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving final datasets: {e}\")\n",
    "\n",
    "        # Save models and results\n",
    "        print(f\"\\nBest model saved to ./self_trained_model_iter_*\")\n",
    "        \n",
    "        # Save training history\n",
    "        with open(\"self_training_history.json\", \"w\") as f:\n",
    "            json.dump(performance_history, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nSelf-training pipeline with real annotated data completed!\")\n",
    "        print(f\"Complete training log saved to: {output_filename}\")\n",
    "        print(f\"Training ended at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nFATAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        # Always restore stdout and close log file\n",
    "        sys.stdout = original_stdout\n",
    "        output_logger.close()\n",
    "        print(f\"\\nTraining output saved to: {output_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
